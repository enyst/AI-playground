{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enyst/AI-playground/blob/main/templates/aistudio_gemini_prompt_freeform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "kWIuwKG2_oWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b370dd0-83b5-45c3-afcd-963f0313e430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"Explain SGD in the ML field, for an audience of software developers who have interacted with pretrained LLMs, but do not necessarily have the technical knowledge of the full training process. Keep it short and sweet. Do not infantilize the audience.\\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"model\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"Imagine fine-tuning a pre-trained LLM.  You have a bunch of examples and you want the model to perform well on them.  SGD, or Stochastic Gradient Descent, is like navigating a foggy mountain.  You want to reach the lowest point (best model performance), but you can only see a few feet around you.\\\\n\\\\nEach \\\\\\\"step\\\\\\\" in SGD involves:\\\\n\\\\n1. Picking a small, random batch of your examples.\\\\n2. Calculating how \\\\\\\"wrong\\\\\\\" the model is on those examples (the \\\\\\\"loss\\\\\\\").\\\\n3. Slightly adjusting the model's internal parameters (\\\\\\\"weights\\\\\\\") to hopefully reduce the loss on similar examples.\\\\n\\\\nThis process repeats, iteratively \\\\\\\"stepping\\\\\\\" downhill towards better performance.  \\\\\\\"Stochastic\\\\\\\" means we're using small random batches instead of the whole dataset at once, which makes it faster and allows the model to generalize better.  \\\\\\\"Gradient\\\\\\\" refers to the direction of steepest descent in the loss landscape.  \\\\\\\"Descent\\\\\\\" means we're moving towards lower loss.\\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "    # The SDK will automatically read it from the GOOGLE_API_KEY environment variable.\n",
        "    # In Colab get the key from Colab-secrets (\"üîë\" in the left panel).\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Parse the arguments\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # @param {isTemplate: true}\n",
        "contents_b64 = 'W3sicm9sZSI6InVzZXIiLCJwYXJ0cyI6W3sidGV4dCI6IkV4cGxhaW4gU0dEIGluIHRoZSBNTCBmaWVsZCwgZm9yIGFuIGF1ZGllbmNlIG9mIHNvZnR3YXJlIGRldmVsb3BlcnMgd2hvIGhhdmUgaW50ZXJhY3RlZCB3aXRoIHByZXRyYWluZWQgTExNcywgYnV0IGRvIG5vdCBuZWNlc3NhcmlseSBoYXZlIHRoZSB0ZWNobmljYWwga25vd2xlZGdlIG9mIHRoZSBmdWxsIHRyYWluaW5nIHByb2Nlc3MuIEtlZXAgaXQgc2hvcnQgYW5kIHN3ZWV0LiBEbyBub3QgaW5mYW50aWxpemUgdGhlIGF1ZGllbmNlLlxcbiJ9XX0seyJyb2xlIjoibW9kZWwiLCJwYXJ0cyI6W3sidGV4dCI6IkltYWdpbmUgZmluZS10dW5pbmcgYSBwcmUtdHJhaW5lZCBMTE0uICBZb3UgaGF2ZSBhIGJ1bmNoIG9mIGV4YW1wbGVzIGFuZCB5b3Ugd2FudCB0aGUgbW9kZWwgdG8gcGVyZm9ybSB3ZWxsIG9uIHRoZW0uICBTR0QsIG9yIFN0b2NoYXN0aWMgR3JhZGllbnQgRGVzY2VudCwgaXMgbGlrZSBuYXZpZ2F0aW5nIGEgZm9nZ3kgbW91bnRhaW4uICBZb3Ugd2FudCB0byByZWFjaCB0aGUgbG93ZXN0IHBvaW50IChiZXN0IG1vZGVsIHBlcmZvcm1hbmNlKSwgYnV0IHlvdSBjYW4gb25seSBzZWUgYSBmZXcgZmVldCBhcm91bmQgeW91LlxcblxcbkVhY2ggXFxcInN0ZXBcXFwiIGluIFNHRCBpbnZvbHZlczpcXG5cXG4xLiBQaWNraW5nIGEgc21hbGwsIHJhbmRvbSBiYXRjaCBvZiB5b3VyIGV4YW1wbGVzLlxcbjIuIENhbGN1bGF0aW5nIGhvdyBcXFwid3JvbmdcXFwiIHRoZSBtb2RlbCBpcyBvbiB0aG9zZSBleGFtcGxlcyAodGhlIFxcXCJsb3NzXFxcIikuXFxuMy4gU2xpZ2h0bHkgYWRqdXN0aW5nIHRoZSBtb2RlbCdzIGludGVybmFsIHBhcmFtZXRlcnMgKFxcXCJ3ZWlnaHRzXFxcIikgdG8gaG9wZWZ1bGx5IHJlZHVjZSB0aGUgbG9zcyBvbiBzaW1pbGFyIGV4YW1wbGVzLlxcblxcblRoaXMgcHJvY2VzcyByZXBlYXRzLCBpdGVyYXRpdmVseSBcXFwic3RlcHBpbmdcXFwiIGRvd25oaWxsIHRvd2FyZHMgYmV0dGVyIHBlcmZvcm1hbmNlLiAgXFxcIlN0b2NoYXN0aWNcXFwiIG1lYW5zIHdlJ3JlIHVzaW5nIHNtYWxsIHJhbmRvbSBiYXRjaGVzIGluc3RlYWQgb2YgdGhlIHdob2xlIGRhdGFzZXQgYXQgb25jZSwgd2hpY2ggbWFrZXMgaXQgZmFzdGVyIGFuZCBhbGxvd3MgdGhlIG1vZGVsIHRvIGdlbmVyYWxpemUgYmV0dGVyLiAgXFxcIkdyYWRpZW50XFxcIiByZWZlcnMgdG8gdGhlIGRpcmVjdGlvbiBvZiBzdGVlcGVzdCBkZXNjZW50IGluIHRoZSBsb3NzIGxhbmRzY2FwZS4gIFxcXCJEZXNjZW50XFxcIiBtZWFucyB3ZSdyZSBtb3ZpbmcgdG93YXJkcyBsb3dlciBsb3NzLlxcbiJ9XX1d'  # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MSwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9'  # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False\n",
        "\n",
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"üìÅ Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zAD69vE92b"
      },
      "source": [
        "## Call `generate_content`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LB2LxPmAB95V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "dd9c4ea9-43e2-4c87-a5cf-5397ff5a64ee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Think of it as a slow, controlled learning process.  SGD helps the LLM learn from its mistakes, gradually improving its performance on your specific examples.  It's the workhorse of training deep learning models, and the foundation of many advancements in AI. \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9d345e9868"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Docs on ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-gemini/cookbook/blob/main/quickstarts\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />More notebooks in the Cookbook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91AeeGO1ncU"
      },
      "source": [
        "## [optional] Show the conversation\n",
        "\n",
        "This section displays the conversation received from Google AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "yoL3p3KPylFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f1522a-4b3a-4160-acdf-b6cd7c58a974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user \n",
            "\n",
            "Explain SGD in the ML field, for an audience of software developers who have interacted with pretrained LLMs, but do not necessarily have the technical knowledge of the full training process. Keep it short and sweet. Do not infantilize the audience.\\n \n",
            "\n",
            "-------------------------------------------------------------------------------- \n",
            "\n",
            "Role: model \n",
            "\n",
            "Imagine fine-tuning a pre-trained LLM.  You have a bunch of examples and you want the model to perform well on them.  SGD, or Stochastic Gradient Descent, is like navigating a foggy mountain.  You want to reach the lowest point (best model performance), but you can only see a few feet around you.\\n\\nEach \\\"step\\\" in SGD involves:\\n\\n1. Picking a small, random batch of your examples.\\n2. Calculating how \\\"wrong\\\" the model is on those examples (the \\\"loss\\\").\\n3. Slightly adjusting the model's internal parameters (\\\"weights\\\") to hopefully reduce the loss on similar examples.\\n\\nThis process repeats, iteratively \\\"stepping\\\" downhill towards better performance.  \\\"Stochastic\\\" means we're using small random batches instead of the whole dataset at once, which makes it faster and allows the model to generalize better.  \\\"Gradient\\\" refers to the direction of steepest descent in the loss landscape.  \\\"Descent\\\" means we're moving towards lower loss.\\n \n",
            "\n",
            "-------------------------------------------------------------------------------- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Show the conversation, in colab.\n",
        "import mimetypes\n",
        "\n",
        "def show_file(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "\n",
        "    if drive_id := file_data.get(\"drive_id\", None):\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        name = path\n",
        "        # data = path.read_bytes()\n",
        "        kwargs = {\"filename\": path}\n",
        "    elif url := file_data.get(\"url\", None):\n",
        "        name = url\n",
        "        kwargs = {\"url\": url}\n",
        "        # response = requests.get(url)\n",
        "        # data = response.content\n",
        "    elif data := file_data.get(\"inline_data\", None):\n",
        "        name = None\n",
        "        kwargs = {\"data\": data}\n",
        "    elif name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files to \"\n",
        "                'Colab using the file manager (\"üìÅ Files\"in the left toolbar)'\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "        print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "        return\n",
        "\n",
        "    format = mimetypes.guess_extension(mime_type).strip(\".\")\n",
        "    if mime_type.startswith(\"image/\"):\n",
        "        image = IPython.display.Image(**kwargs, width=256)\n",
        "        IPython.display.display(image)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if mime_type.startswith(\"audio/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Audio(**kwargs)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    if mime_type.startswith(\"video/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Video(**kwargs, mimetype=mime_type)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "\n",
        "\n",
        "for content in gais_contents:\n",
        "    if role := content.get(\"role\", None):\n",
        "        print(\"Role:\", role, \"\\n\")\n",
        "\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if text := part.get(\"text\", None):\n",
        "            print(text, \"\\n\")\n",
        "\n",
        "        elif file_data := part.get(\"file_data\", None):\n",
        "            show_file(file_data)\n",
        "\n",
        "    print(\"-\" * 80, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}